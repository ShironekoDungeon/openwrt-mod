diff -Naur a/src/string/memcpy.c b/src/string/memcpy.c
--- a/src/string/memcpy.c	2017-01-03 00:47:12.000000000 +0000
+++ b/src/string/memcpy.c	2017-08-06 02:15:18.000000000 +0000
@@ -2,6 +2,85 @@
 #include <stdint.h>
 #include <endian.h>
 
+#ifdef _ARCH_PPC
+/*
+ * Copyright (C) 2004 Joakim Tjernlund
+ * Copyright (C) 2000-2005 Erik Andersen <andersen@uclibc.org>
+ *
+ * Licensed under the LGPL v2.1, see the file COPYING.LIB in this tarball.
+ *
+ * These are carefully optimized mem*() functions for PPC written in C.
+ * Don't muck around with these function without checking the generated
+ * assembler code.
+ * It is possible to optimize these significantly more by using specific
+ * data cache instructions(mainly dcbz). However that requires knownledge
+ * about the CPU's cache line size.
+ *
+ * BUG ALERT!
+ * The cache instructions on MPC8xx CPU's are buggy(they don't update
+ * the DAR register when causing a DTLB Miss/Error) and cannot be
+ * used on 8xx CPU's without a kernel patch to work around this
+ * problem.
+ */
+/* PPC can do pre increment and load/store, but not post increment and
+   load/store.  Therefore use *++ptr instead of *ptr++.  */
+void *memcpy(void *restrict to, const void *restrict from, size_t len)
+{
+	unsigned long rem, chunks, tmp1, tmp2;
+	unsigned char *tmp_to = to;
+	const unsigned char *tmp_from = from;
+
+	chunks = len / 8;
+	tmp_from -= 4;
+	tmp_to -= 4;
+	if (!chunks)
+		goto lessthan8;
+	rem = (unsigned long )tmp_to % 4;
+	if (rem)
+		goto align;
+ copy_chunks:
+	do {
+		/* make gcc to load all data, then store it */
+		tmp1 = *(unsigned long *)(tmp_from+4);
+		tmp_from += 8;
+		tmp2 = *(unsigned long *)tmp_from;
+		*(unsigned long *)(tmp_to+4) = tmp1;
+		tmp_to += 8;
+		*(unsigned long *)tmp_to = tmp2;
+	} while (--chunks);
+ lessthan8:
+	len = len % 8;
+	if (len >= 4) {
+		tmp_from += 4;
+		tmp_to += 4;
+		*(unsigned long *)(tmp_to) = *(unsigned long *)(tmp_from);
+		len -= 4;
+	}
+	if (!len)
+		return to;
+	tmp_from += 3;
+	tmp_to += 3;
+	do {
+		*++tmp_to = *++tmp_from;
+	} while (--len);
+
+	return to;
+ align:
+	/* ???: Do we really need to generate the carry flag here? If not, then:
+	rem -= 4; */
+	rem = 4 - rem;
+	len -= rem;
+	do {
+		*(tmp_to+4) = *(tmp_from+4);
+		++tmp_from;
+		++tmp_to;
+	} while (--rem);
+	chunks = len / 8;
+	if (chunks)
+		goto copy_chunks;
+	goto lessthan8;
+}
+#else
 void *memcpy(void *restrict dest, const void *restrict src, size_t n)
 {
 	unsigned char *d = dest;
@@ -122,3 +201,4 @@
 	for (; n; n--) *d++ = *s++;
 	return dest;
 }
+#endif
\ No newline at end of file
diff -Naur a/src/string/memmove.c b/src/string/memmove.c
--- a/src/string/memmove.c	2017-01-03 00:47:12.000000000 +0000
+++ b/src/string/memmove.c	2017-08-06 02:18:05.000000000 +0000
@@ -3,7 +3,76 @@
 
 #define WT size_t
 #define WS (sizeof(WT))
-
+#ifdef _ARCH_PPC
+/*
+ * Copyright (C) 2004 Joakim Tjernlund
+ * Copyright (C) 2000-2005 Erik Andersen <andersen@uclibc.org>
+ *
+ * Licensed under the LGPL v2.1, see the file COPYING.LIB in this tarball.
+ *
+ * These are carefully optimized mem*() functions for PPC written in C.
+ * Don't muck around with these function without checking the generated
+ * assembler code.
+ * It is possible to optimize these significantly more by using specific
+ * data cache instructions(mainly dcbz). However that requires knownledge
+ * about the CPU's cache line size.
+ *
+ * BUG ALERT!
+ * The cache instructions on MPC8xx CPU's are buggy(they don't update
+ * the DAR register when causing a DTLB Miss/Error) and cannot be
+ * used on 8xx CPU's without a kernel patch to work around this
+ * problem.
+ */
+void *memmove(void *to, const void *from, size_t n)
+{
+	unsigned long rem, chunks, tmp1, tmp2;
+	unsigned char *tmp_to = to;
+	unsigned char *tmp_from = (unsigned char *)from;
+	if (tmp_from >= (unsigned char *)to)
+		return memcpy(to, from, n);
+	chunks = n / 8;
+	tmp_from += n;
+	tmp_to += n;
+	if (!chunks)
+		goto lessthan8;
+	rem = (unsigned long )tmp_to % 4;
+	if (rem)
+		goto align;
+ copy_chunks:
+	do {
+		/* make gcc to load all data, then store it */
+		tmp1 = *(unsigned long *)(tmp_from-4);
+		tmp_from -= 8;
+		tmp2 = *(unsigned long *)tmp_from;
+		*(unsigned long *)(tmp_to-4) = tmp1;
+		tmp_to -= 8;
+		*(unsigned long *)tmp_to = tmp2;
+	} while (--chunks);
+ lessthan8:
+	n = n % 8;
+	if (n >= 4) {
+		*(unsigned long *)(tmp_to-4) = *(unsigned long *)(tmp_from-4);
+		tmp_from -= 4;
+		tmp_to -= 4;
+		n = n-4;
+}
+	if (!n ) return to;
+	do {
+		*--tmp_to = *--tmp_from;
+	} while (--n);
+	return to;
+ align:
+	rem = 4 - rem;
+	n = n - rem;
+	do {
+		*--tmp_to = *--tmp_from;
+	} while (--rem);
+	chunks = n / 8;
+	if (chunks)
+		goto copy_chunks;
+	goto lessthan8;
+}
+#else
 void *memmove(void *dest, const void *src, size_t n)
 {
 	char *d = dest;
@@ -34,3 +103,4 @@
 
 	return dest;
 }
+#endif
\ No newline at end of file
diff -Naur a/src/string/memset.c b/src/string/memset.c
--- a/src/string/memset.c	2017-01-03 00:47:12.000000000 +0000
+++ b/src/string/memset.c	2017-08-06 02:22:26.000000000 +0000
@@ -1,6 +1,83 @@
 #include <string.h>
 #include <stdint.h>
 
+#ifdef _ARCH_PPC
+/*
+ * Copyright (C) 2004 Joakim Tjernlund
+ * Copyright (C) 2000-2005 Erik Andersen <andersen@uclibc.org>
+ *
+ * Licensed under the LGPL v2.1, see the file COPYING.LIB in this tarball.
+ *
+ * These are carefully optimized mem*() functions for PPC written in C.
+ * Don't muck around with these function without checking the generated
+ * assembler code.
+ * It is possible to optimize these significantly more by using specific
+ * data cache instructions(mainly dcbz). However that requires knownledge
+ * about the CPU's cache line size.
+ *
+ * BUG ALERT!
+ * The cache instructions on MPC8xx CPU's are buggy(they don't update
+ * the DAR register when causing a DTLB Miss/Error) and cannot be
+ * used on 8xx CPU's without a kernel patch to work around this
+ * problem.
+ */
+static __inline__ int expand_byte_word(int c){
+	/* this does:
+	   c = c << 8 | c;
+	   c = c << 16 | c ;
+	*/
+	__asm__("rlwimi	%0,%0,8,16,23\n"
+	    "\trlwimi	%0,%0,16,0,15\n"
+	    : "=r" (c) : "0" (c));
+	return c;
+}
+
+void *memset(void *to, int c, size_t n)
+{
+	unsigned long rem, chunks;
+	unsigned char *tmp_to = to;
+	chunks = n / 8;
+	tmp_to -= 4;
+	c = expand_byte_word(c);
+	if (!chunks)
+		goto lessthan8;
+	rem = (unsigned long )tmp_to % 4;
+	if (rem)
+		goto align;
+ copy_chunks:
+	do {
+		*(unsigned long *)(tmp_to+4) = c;
+		tmp_to += 4;
+		*(unsigned long *)(tmp_to+4) = c;
+		tmp_to += 4;
+	} while (--chunks);
+ lessthan8:
+	n = n % 8;
+	if (n >= 4) {
+		*(unsigned long *)(tmp_to+4) = c;
+		tmp_to += 4;
+		n = n-4;
+}
+	if (!n ) return to;
+	tmp_to += 3;
+	do {
+		*++tmp_to = c;
+	} while (--n);
+
+	return to;
+ align:
+	rem = 4 - rem;
+	n = n-rem;
+	do {
+		*(tmp_to+4) = c;
+		++tmp_to;
+	} while (--rem);
+	chunks = n / 8;
+	if (chunks)
+		goto copy_chunks;
+	goto lessthan8;
+}
+#else
 void *memset(void *dest, int c, size_t n)
 {
 	unsigned char *s = dest;
@@ -84,3 +161,4 @@
 
 	return dest;
 }
+#endif
\ No newline at end of file
